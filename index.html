<!DOCTYPE html>
<html>
<head>
  <meta name=viewport content="width=device-width, initial-scale=1">
  <title>Jared Dunbar - MP251 sp2017</title>
  <link rel="stylesheet" type="text/css" href="style.css">
  <script type="text/javascript">

function stp() {
  audio.pause();
}

</script>
</head>
<body>
  <!-- check out dem comments. Ironicly, half of the content is probably notes in the document and not in the presentation, but having information about how stuff do is important for posterity. -->
  <div>
    <h1>Jared Dunbar - Spring 2017</h1>
    <p>
      COSI for credit - 2 credit hours <!-- Dangit! Should have done three! Next semester...  -->
    </p>
  </div>
  <div>
    <h2>COSI Stuff</h2>
    <p>
      Kept the lab clean, participated in all meetings that I could get to (exception being I can't miss exams<!-- Thanks Maciel.. really the students of our class because they insisted that it was the /best/ time -->).
    </p>
    <p>
      Worked Accepted Students Day, Open House, etc. Cleaned the room, taught potential students about the university, general aid to new potential students.
    </p>
    <p>
      Volunteered for Hack Potsdam, did setup and stayed through the night.
    </p>
    <p>
      Gave 5 whole lightning talks:
    </p>
    <ul>
      <li><a href="http://adventurega.me/bootstrap/" target="_blank">Every Bootstrap Site Ever</a></li>
      <li><a href="http://web2.clarkson.edu/projects/cosi/sp2017/students/dunbarj/power.html" target="_blank">Raspberry Pi Power Usage</a></li>
      <li><a href="http://shattered.io/" target="_blank">Google "breaks" SHA1</a></li>
      <li><a href="http://reddit.com/r/place" target="_blank">/r/place</a></li>
      <li><a href="http://go.ja13.org" target="_blank">GoAccess Web Analytics</a></li>
    </ul>
    <p>
      Gave a small group of 3 workshops titled "How it do?" - we covered:
    </p>
    <ul>
      <li>Server Room operations</li>
      <li>How to maintain VMs</li>
      <li>An Arch Linux install with software raid</li>
    </ul>
    <p>
      Helped in process of depricating and handing out older hardware from the GDC project.
      <!-- Finally ending the "craprack" as it was. That was a ridiculous research project that we hope to have nipped in the bud, never to do again EVER. It involved leadership much worse than when I encountered IgniteCS. The purpose of the research was to see if colocation could be used to use cheap power in different locations to make running servers cheaper (turning off servers where it's expensive, turning it on where it's cheap), but ignored all of the overhead costs, which is why it was awful. In practice, this doesn't even make sense - Google moves containers of servers to the areas near the Superbowl each year, just to handle the shear amount of data, much unlike this cost avoidance research which would make user experiences awful or unusable in many cases where you weren't just running some small website, in such cases using a container would make much more sense. -->
      This hardware was repurposed to interested students who wanted to run honeypots or use it for research.
      <!-- The rule of thumb if it was worth it or not was if it was about equivelant in power to a raspberry pi, or slightly better. Much of it didn't pass this test, nevermind POST. The drives in those machines will either die tomorrow or last forever, and the rest of the hardware was quite jank, requiring some interesting BIOS CD's which can still be found scattered around COSI. Much a pain, since the only practical method to using them in any usable way was working with the PXE system we have going to bootstrap operating systems. -->
    </p>
  </div>
  <div>
    <h2>Projects</h2>
    <p>
      Renamed Management to Stat (because I like how it sounds :D). Working gradually on moving to a new codebase which is partially C/C++. Currently writing on displaying network data in a simple, readable way. Also, wrote some stuff to use JavaScript in the browser instead of expensive/insecure CGI operations.
    </p>
    <p>
      Created a telnet honeypot for Network Security, and collected interesting information on some botnets.
    </p>
    <p>
      Working with Graham to debug bash. We were debugging <a href="http://gmx.cosi.clarkson.edu/" target="_blank">GMx</a> (the VM that is the COSI game server), which we would occasionally have bash crash. When you run gdb on the bash process to debug this issue, when you stepped the instruction pointer, it would crash the entire server, including the VM host. We forwarded a resulting bash bug to the developers of bash, and moved to a version of the kernel where Linux would not misbehave with a certain processor feature.
    </p>
  </div>
  <div>
    <h2>Outreach programs</h2>
    <p>
      Taught three sessions of IgniteCS - the Python Classess and Functions session, the Scratch session, and the GPIO session. The latter two were a huge success - students were extremely excited to work with hardware and create games. Also worked hard to try and unify the Clarkson CS Outreach Program. Filled the position of co-director.
    </p>
  </div>
  <div>
    <h2>Server Room Stuff</h2>
    <h3>Virtual Machines</h3>
    <p>
      Migrated away from Bennu, and installed Grand-Dad. Maintained VMs, and reinstalled Odin and Docs with new Debian versions.
    </p>
    <p>
      Reinstalled Hydra with Arch Linux because it had a more stable kernel than both Debian and Ubuntu at the time. Stability&#8482;
    </p>
    <h3>Network Stuff</h3>
    <p>
      Replaced main COSI network switch which was dead.
    </p>
    <p>
      Configured network switches to have bonded interfaces with the two VM hosts. It is now possible to transfer files between the two VM hosts at up to 2 Gigabit.
    </p>
    <p>
      Communicated with OIT about reducing our ARP traffic. Come to find out, there's not much we can do. ~25kb/s is going to be the norm from now on for ARP traffic. A way to resolve this issue would be to become firewalled, but we strongly suggest against it. It's much easier for us to internally firewall on both Ziltoid and then on the clients, and disregard the ARP. Perhaps ARP cache in the future would make more sense...
    </p>
  </div>
  <div id="music">
    <audio src="http://gmx.cosi.clarkson.edu/music/tricks.mp3" style="display:hidden" autoplay></audio>
  </div>
  <!-- Fun fact! I plan to do some infastructure upgrades over the summer of 2017, such as checking all of the networking in the ITL and then trying LAG for some of the switches to set that up in a better state (especially when we have lots of data from the server room to the ITL). We often hit the operational ceiling of our network's current 1G network, and since it's hub and spoke, we can't do much about throughput issues at the moment. This also means that when we update the ITL currently, there's not as much traffic throughput for COSI to Clarkson bandwidth, besides Mirror to Clarkson bandwidth, which doesn't get affected. This is much due to the system we have going with Ziltoid being the firewall, but the ITL not being behind it, but also that everything hubs around Ziltoid as it stands. Also, some VLAN's may be in order for the 146 network over the summer, to allow use of the 146 in network security VM's. Many plans to be had. :D -->
</body>
</html>
